@techreport{usda_2020,
  author = {{US Department of Agriculture and US Department of Health and Human Services}},
  title = {Dietary Guidelines for Americans, 2020-2025},
  institution = {{US Department of Agriculture and US Department of Health and Human Services}},
  year = {2020},
  number = {9th edition},
  url = {https://DietaryGuidelines.gov}
}

@techreport{levin_et_al_2018,
  author = {Levin, D. and D. Noriega and C. Dicken and A. Okrent and M. Harding and M. Lovenheim},
  title = {Examining Store Scanner Data: A Comparison of the IRI Infoscan Data with Other Data Sets, 2008-12},
  institution = {US Department of Agriculture, Economic Research Service},
  year = {2018},
  type = {Technical bulletin},
  number = {1949}
}

@techreport{muth_et_al_2016,
  author = {Muth, M. K. and M. Sweitzer and D. Brown and K. Capogrossi and S. Karns and D. Levin and A. Okrent and P. Siegel and C. Zhen},
  title = {Understanding IRI Household-Based and Store-Based Scanner Data},
  institution = {US Department of Agriculture, Economic Research Service},
  year = {2016},
  type = {Technical bulletin},
  number = {1942}
}

@techreport{thrifty_food_plan_2021,
  author = {{US Department of Agriculture}},
  title = {Thrifty Food Plan, 2021},
  institution = {US Department of Agriculture},
  year = {2021},
  type = {Food and Nutrition Service},
  number = {916},
  url = {https://FNS.usda.gov/TFP}
}

@article{gattshall_et_al_2008,
  author = {Gattshall, M. L. and J. A. Shoup and J. A. Marshall and L. A. Crane and P. A. Estabrooks},
  title = {Validation of a survey instrument to assess home environments for physical activity and healthy eating in overweight children},
  journal = {International Journal of Behavioral Nutrition and Physical Activity},
  year = {2008},
  volume = {5},
  number = {3},
  doi = {10.1186/1479-5868-5-3}
}

@article{hanson_et_al_2005,
  author = {Hanson, N. I. and D. Neumark-Sztainer and M. E. Eisenberg and M. Story and M. Wall},
  title = {Associations between parental report of the home food environment and adolescent intakes of fruits, vegetables and dairy foods},
  journal = {Public Health Nutrition},
  year = {2005},
  volume = {8},
  number = {1},
  doi = {10.1079/PHN2005661}
}

@article{REEDY2014881,
  title = {Higher Diet Quality Is Associated with Decreased Risk of All-Cause, Cardiovascular Disease, and Cancer Mortality among Older Adults},
  journal = {The Journal of Nutrition},
  volume = {144},
  number = {6},
  pages = {881-889},
  year = {2014},
  issn = {0022-3166},
  doi = {10.3945/jn.113.189407},
  author = {J. Reedy and S. M. Krebs-Smith and P. E. Miller and A. D. Liese and L. L. Kahle and Y. Park and A. F. Subar},
}

@conference{cleary_et_al_2022,
  author = {Cleary, R. and Liu, Y. and Carlson, A.},
  title = {Differences in the Distribution of Nutrition Between Households Above and Below Poverty},
  series = {Agricultural and Applied Economic Association Annual Meeting},
  address = {Anaheim, CA.},
  year = {2022},
  url = {https://ageconsearch.umn.edu/record/322267}
}

@techreport{carlson_et_al_2019,
  author = {Carlson, A. C. and E. T. Page and T. P. Zimmerman and C. E. Tornow and S. Hermansen},
  title = {Linking USDA Nutrition Databases to IRI Household-Based and Store-Based Scanner Data},
  institution = {US Department of Agriculture, Economic Research Service},
  year = {2019},
  type = {Technical bulletin},
  number = {1952}
}

@article{carlson_et_al_2022,
  author = {Carlson, A. C. and C. E. Tornow and E. T. Page and A. Brown McFadden and T. Palmer Zimmerman},
  title = {Development of the Purchase to Plate Crosswalk and Price Tool: Estimating Prices for the National Health and Nutrition Examination Survey (NHANES) Foods and Measuring the Healthfulness of Retail Food Purchases},
  journal = {Journal of Food Composition and Analysis},
  year = {2022},
  volume = {106},
  pages = {104344},
  doi = {10.1016/j.jfca.2021.104344}
}

@techreport{fndds_2018,
  author = {{US Department of Agriculture, Agricultural Research Service}},
  title = {USDA Food and Nutrient Database for Dietary Studies 2015-2016},
  year = {2018},
  institution = {US Department of Agriculture, Agricultural Research Service},
  url = {https://www.ars.usda.gov/nea/bhnrc/fsrg}
}

@techreport{fndds_2020,
  author = {{US Department of Agriculture, Agricultural Research Service}},
  title = {USDA Food and Nutrient Database for Dietary Studies 2017-2018},
  year = {2020},
  institution = {US Department of Agriculture, Agricultural Research Service},
  url = {https://www.ars.usda.gov/nea/bhnrc/fsrg}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author = {J. Devlin and M.{-}W. Chang and K. Lee and K. Toutanova},
  title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal = {CoRR},
  volume = {abs/1810.04805},
  year = {2018},
  url = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Conference{macavaney_et_al_2021,
  author = {Macavaney, S. and Mittu, A. and Coppersmith, G. and Leintz, J. and Resnik, P.},
  title = {Community-level research on suicidality prediction in a secure environment: Overview of the CLPsych 2021 shared task},
  series = {In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access},
  year = {2021},
  pages = {70-80}
}

@InProceedings{10.1007/978-3-030-03146-6_86,
  author = {Parmar, A. and Katariya, R. and Patel, V.},
  editor = {Hemanth, J. and Fernando, X. and Lafata, P. and Baig, Z.},
  title = {A Review on Random Forest: An Ensemble Classifier},
  booktitle = {International Conference on Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018},
  year = {2019},
  publisher = {Springer International Publishing},
  address = {Cham},
  pages = {758--763},
  abstract = {Ensemble classification is an information mining approach which utilizes various classifiers that cooperate for distinguishing the class label for new unlabeled thing from accumulation. Arbitrary Forest approach joins a few randomized choice trees and totals their forecasts by averaging. It has grabbed well-known attention from the community of research because of its high accuracy and superiority which additionally increase the performance. Now in this paper, we take a gander at improvements of Random Forest from history to till date. Our approach is to take a recorded view on the improvement of this prominently effective classification procedure. To begin with history of Random Forest to main technique proposed by Breiman then successful applications that utilized Random Forest and finally some comparison with other classifiers. This paper is proposed to give non specialists simple access to the principle thoughts of random forest.},
  isbn = {978-3-030-03146-6}
}

@misc{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space}, 
  author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  year={2013},
  eprint={1301.3781},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{pennington-etal-2014-glove,
  title = {{G}lo{V}e: Global Vectors for Word Representation},
  author = {Pennington, J. and Socher, R. and Manning, C.},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  month = oct,
  year = {2014},
  address = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/D14-1162},
  doi = {10.3115/v1/D14-1162},
  pages = {1532--1543}
}

@article{SCHMIDHUBER201585,
  title = {Deep learning in neural networks: An overview},
  journal = {Neural Networks},
  volume = {61},
  pages = {85-117},
  year = {2015},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
  author = {J. Schmidhuber},
  keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@article{10.5120/ijca2017915495,
	author = {K. Potdar and T. S. Pardawala and C. D. Pai},
	title = {A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers},
	journal = {International Journal of Computer Applications},
	issue_date = {October 2017},
	volume = {175},
	number = {4},
	month = {Oct},
	year = {2017},
	issn = {0975-8887},
	pages = {7-9},
	numpages = {3},
	url = {http://www.ijcaonline.org/archives/volume175/number4/28474-2017915495},
	doi = {10.5120/ijca2017915495},
	publisher = {Foundation of Computer Science (FCS), NY, USA},
	address = {New York, USA}
}

@inproceedings{10.1145/3443279.3443304,
  author = {Wang, C. and Nulty, P. and Lillis, D.},
  title = {A Comparative Study on Word Embeddings in Deep Learning for Text Classification},
  year = {2021},
  isbn = {9781450377607},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3443279.3443304},
  doi = {10.1145/3443279.3443304},
  abstract = {Word embeddings act as an important component of deep models for providing input features in downstream language tasks, such as sequence labelling and text classification. In the last decade, a substantial number of word embedding methods have been proposed for this purpose, mainly falling into the categories of classic and context-based word embeddings. In this paper, we conduct controlled experiments to systematically examine both classic and contextualised word embeddings for the purposes of text classification. To encode a sequence from word representations, we apply two encoders, namely CNN and BiLSTM, in the downstream network architecture. To study the impact of word embeddings on different datasets, we select four benchmarking classification datasets with varying average sample length, comprising both single-label and multi-label classification tasks. The evaluation results with confidence intervals indicate that CNN as the downstream encoder outperforms BiLSTM in most situations, especially for document context-insensitive datasets. This study recommends choosing CNN over BiLSTM for document classification datasets where the context in sequence is not as indicative of class membership as sentence datasets. For word embeddings, concatenation of multiple classic embeddings or increasing their size does not lead to a statistically significant difference in performance despite a slight improvement in some cases. For context-based embeddings, we studied both ELMo and BERT. The results show that BERT overall outperforms ELMo, especially for long document datasets. Compared with classic embeddings, both achieve an improved performance for short datasets while the improvement is not observed in longer datasets.},
  booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
  pages = {37–46},
  numpages = {10},
  keywords = {Text Classification, Neural Networks, Word Embeddings},
  location = {Seoul, Republic of Korea},
  series = {NLPIR '20}
}

@article{aly2005survey,
  title={Survey on multiclass classification methods, Tech. Rep},
  author={Aly, M},
  journal={California Institute of Technology},
  year={2005}
}

@inproceedings{10.1145/967900.968151,
  author = {Qian, G. and Sural, S. and Gu, Y. and Pramanik, S.},
  title = {Similarity between Euclidean and Cosine Angle Distance for Nearest Neighbor Queries},
  year = {2004},
  isbn = {1581138121},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/967900.968151},
  doi = {10.1145/967900.968151},
  abstract = {Understanding the relationship among different distance measures is helpful in choosing a proper one for a particular application. In this paper, we compare two commonly used distance measures in vector models, namely, Euclidean distance (EUD) and cosine angle distance (CAD), for nearest neighbor (NN) queries in high dimensional data spaces. Using theoretical analysis and experimental results, we show that the retrieval results based on EUD are similar to those based on CAD when dimension is high. We have applied CAD for content based image retrieval (CBIR). Retrieval results show that CAD works no worse than EUD, which is a commonly used distance measure for CBIR, while providing other advantages, such as naturally normalized distance.},
  booktitle = {Proceedings of the 2004 ACM Symposium on Applied Computing},
  pages = {1232–1237},
  numpages = {6},
  keywords = {Cosine angle distance, vector model, Content based image retrieval, Euclidean distance, Inter-feature normalization},
  location = {Nicosia, Cyprus},
  series = {SAC '04}
}

@article{DBLP:journals/corr/PangLGXWC16,
  author = {L. Pang and Y. Lan and J. Guo and J. Xu and S. Wan and X. Cheng},
  title = {Text Matching as Image Recognition},
  journal = {CoRR},
  volume = {abs/1602.06359},
  year = {2016},
  url = {http://arxiv.org/abs/1602.06359},
  eprinttype = {arXiv},
  eprint = {1602.06359},
  timestamp = {Mon, 13 Aug 2018 16:47:25 +0200},
  biburl = {https://dblp.org/rec/journals/corr/PangLGXWC16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/3-540-45014-9_1,
  author = {Dietterich, T. G.},
  title = {Ensemble Methods in Machine Learning},
  booktitle = {Multiple Classifier Systems},
  year = {2000},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {1--15},
  abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
  isbn = {978-3-540-45014-6}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author = {Y. Liu and M. Ott and N. Goyal and J. Du and M. Joshi and D. Chen and O. Levy and M. Lewis and L. Zettlemoyer and V. Stoyanov},
  title = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal = {CoRR},
  volume = {abs/1907.11692},
  year = {2019},
  url = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xiao-etal-2019-label,
  title = {Label-Specific Document Representation for Multi-Label Text Classification},
  author = {Xiao, L. and Huang, X. and Chen, B. and Jing, L.},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month = nov,
  year = {2019},
  address = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/D19-1044},
  doi = {10.18653/v1/D19-1044},
  pages = {466--475},
  abstract = {Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts, an adaptive fusion strategy is proposed, which can effectively output the comprehensive label-specific document representation to build multi-label text classifier. Extensive experimental results demonstrate that LSAN consistently outperforms the state-of-the-art methods on four different datasets, especially on the prediction of low-frequency labels. The code and hyper-parameter settings are released to facilitate other researchers.},
}

@INPROCEEDINGS{7953245,
  author = {Korpusik, M. and Collins, Z. and Glass, J.},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title = {Semantic mapping of natural language input to database entries via convolutional neural networks}, 
  year = {2017},
  volume = {},
  number = {},
  pages = {5685-5689},
  doi = {10.1109/ICASSP.2017.7953245}
}

@article{DBLP:journals/corr/abs-1802-05365,
  author = {M. E. Peters and M. Neumann and M. Iyyer and M. Gardner and C. Clark and K. Lee and L. Zettlemoyer},
  title = {Deep contextualized word representations},
  journal = {CoRR},
  volume = {abs/1802.05365},
  year = {2018},
  url = {http://arxiv.org/abs/1802.05365},
  eprinttype = {arXiv},
  eprint = {1802.05365},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{radford2018improving,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, A. and Narasimhan, K. and Salimans, T. and Sutskever, I.},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018,
  url = {https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, \L. and Polosukhin, I.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{NEURIPS2019_dc6a7e65,
 author = {Yang, Z. and Dai, Z. and Yang, Y. and Carbonell, J. and Salakhutdinov, R. R. and Le, Q. V.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{DBLP:journals/corr/abs-1909-11942,
  author = {Z. Lan and M. Chen and S. Goodman and K. Gimpel and P. Sharma and R. Soricut},
  title = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language Representations},
  journal = {CoRR},
  volume = {abs/1909.11942},
  year = {2019},
  url = {http://arxiv.org/abs/1909.11942},
  eprinttype = {arXiv},
  eprint = {1909.11942},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{7078635,
  author = {Korpusik, M. and Schmidt, N. and Drexler, J. and Cyphers, S. and Glass, J.},
  booktitle = {2014 IEEE Spoken Language Technology Workshop (SLT)}, 
  title = {Data collection and language understanding of food descriptions}, 
  year = {2014},
  volume = {},
  number = {},
  pages = {560-565},
  doi = {10.1109/SLT.2014.7078635}
}

@INPROCEEDINGS{7472843,
  author = {Korpusik, M. and Huang, C. and Price, M. and Glass, J.},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title = {Distributional semantics for understanding spoken meal descriptions}, 
  year = {2016},
  volume = {},
  number = {},
  pages = {6070-6074},
  doi = {10.1109/ICASSP.2016.7472843}
}

@ARTICLE{7902155,
  author = {Korpusik, M. and Glass, J.},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title = {Spoken Language Understanding for a Nutrition Dialogue System}, 
  year = {2017},
  volume = {25},
  number = {7},
  pages = {1450-1461},
  doi = {10.1109/TASLP.2017.2694699}
  }

@inproceedings{korpusik17_interspeech,
  author = {M. Korpusik and Z. Collins and J. Glass},
  title = {{Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions}},
  year = 2017,
  booktitle = {Proceedings of Interspeech},
  pages = {3320--3324},
  doi = {10.21437/Interspeech.2017-422}
}

@INPROCEEDINGS{8461769,
  author = {Korpusik, M. and Glass, J.},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title = {Convolutional Neural Networks and Multitask Strategies for Semantic Mapping of Natural Language Input to a Structured Database}, 
  year = {2018},
  volume = {},
  number = {},
  pages = {6174-6178},
  doi = {10.1109/ICASSP.2018.8461769}
}

@ARTICLE{8721137,
  author = {Korpusik, M. and Glass, J.},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title = {Deep Learning for Database Mapping and Asking Clarification Questions in Dialogue Systems}, 
  year = {2019},
  volume = {27},
  number = {8},
  pages = {1321-1334},
  doi = {10.1109/TASLP.2019.2918618}
}

@inproceedings{korpusik19_interspeech,
  author = {M. Korpusik and Z. Liu and J. Glass},
  title = {{A Comparison of Deep Learning Methods for Language Understanding}},
  year = 2019,
  booktitle = {Proceedings of Interspeech},
  pages = {849--853},
  doi = {10.21437/Interspeech.2019-1262}
}

@Book{dong_liu,
  editor = {Dong, G. and Liu, H.},
  title = {Feature Engineering for Machine Learning and Data Analytics},
  publisher = {CRC Press},
  year = {2018},
  edition = {first edition},
}

@article{DBLP:journals/corr/abs-1711-05101,
  author = {I. Loshchilov and F. Hutter},
  title = {Fixing Weight Decay Regularization in Adam},
  journal = {CoRR},
  volume = {abs/1711.05101},
  year = {2017},
  url = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1908-03265,
  author = {L. Liu and H. Jiang and P. He and W. Chen and X. Liu and J. Gao and J. Han},
  title = {On the Variance of the Adaptive Learning Rate and Beyond},
  journal = {CoRR},
  volume = {abs/1908.03265},
  year = {2019},
  url = {http://arxiv.org/abs/1908.03265},
  eprinttype = {arXiv},
  eprint = {1908.03265},
  timestamp = {Mon, 30 May 2022 13:48:56 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1908-03265.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1712-09405,
  author = {T. Mikolov and E. Grave and P. Bojanowski and C. Puhrsch and A. Joulin},
  title = {Advances in Pre-Training Distributed Word Representations},
  journal = {CoRR},
  volume = {abs/1712.09405},
  year = {2017},
  url = {http://arxiv.org/abs/1712.09405},
  eprinttype = {arXiv},
  eprint = {1712.09405},
  timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-1712-09405.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1257/jel.20171350,
  Author = {Christensen, G. and Miguel, E.},
  Title = {Transparency, Reproducibility, and the Credibility of Economics Research},
  Journal = {Journal of Economic Literature},
  Volume = {56},
  Number = {3},
  Year = {2018},
  Month = {September},
  Pages = {920-80},
  DOI = {10.1257/jel.20171350},
  URL = {https://www.aeaweb.org/articles?id=10.1257/jel.20171350}
}

@book{DBLP:books/sp/ChristenRS20,
  author = {P. Christen and T. Ranbaduge and R. Schnell},
  title = {Linking Sensitive Data - Methods and Techniques for Practical Privacy-Preserving Information Sharing},
  publisher = {Springer},
  year = {2020},
  url = {https://doi.org/10.1007/978-3-030-59706-1},
  doi = {10.1007/978-3-030-59706-1},
  isbn = {978-3-030-59705-4},
  timestamp = {Fri, 30 Oct 2020 13:49:37 +0100},
  biburl = {https://dblp.org/rec/books/sp/ChristenRS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2112-01716,
  author = {B. Koch and E. Denton and A. Hanna and J. G. Foster},
  title = {Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research},
  journal = {CoRR},
  volume = {abs/2112.01716},
  year = {2021},
  url = {https://arxiv.org/abs/2112.01716},
  eprinttype = {arXiv},
  eprint = {2112.01716},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-2112-01716.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{43146,
  title	= {Machine Learning: The High Interest Credit Card of Technical Debt},
  author	= {D. Sculley and G. Holt and D. Golovin and E. Davydov and T. Phillips and D. Ebner and V. Chaudhary and M. Young},
  year	= {2014},
  booktitle	= {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)}
}

@article{10.1371/journal.pone.0249833,
  doi = {10.1371/journal.pone.0249833},
  author = {Schafer, K. M. and Kennedy, G. and Gallyer, A. and Resnik, P.},
  journal = {PLOS ONE},
  publisher = {Public Library of Science},
  title = {A direct comparison of theory-driven and machine learning prediction of suicide: A meta-analysis},
  year = {2021},
  month = {04},
  volume = {16},
  url = {https://doi.org/10.1371/journal.pone.0249833},
  pages = {1--23},
  number = {4}
}

@article{10.1007/s11222-017-9746-6,
  author = {Hand, D. and Christen, P.},
  title = {A Note on Using the F-Measure for Evaluating Record Linkage Algorithms},
  year = {2018},
  issue_date = {May 2018},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {28},
  number = {3},
  issn = {0960-3174},
  url = {https://doi.org/10.1007/s11222-017-9746-6},
  doi = {10.1007/s11222-017-9746-6},
  abstract = {Record linkage is the process of identifying and linking records about the same entities from one or more databases. Record linkage can be viewed as a classification problem where the aim is to decide whether a pair of records is a match (i.e. two records refer to the same real-world entity) or a non-match (two records refer to two different entities). Various classification techniques--including supervised, unsupervised, semi-supervised and active learning based--have been employed for record linkage. If ground truth data in the form of known true matches and non-matches are available, the quality of classified links can be evaluated. Due to the generally high class imbalance in record linkage problems, standard accuracy or misclassification rate are not meaningful for assessing the quality of a set of linked records. Instead, precision and recall, as commonly used in information retrieval and machine learning, are used. These are often combined into the popular F-measure, which is the harmonic mean of precision and recall. We show that the F-measure can also be expressed as a weighted sum of precision and recall, with weights which depend on the linkage method being used. This reformulation reveals that the F-measure has a major conceptual weakness: the relative importance assigned to precision and recall should be an aspect of the problem and the researcher or user, but not of the particular linkage method being used. We suggest alternative measures which do not suffer from this fundamental flaw.},
  journal = {Statistics and Computing},
  month = {may},
  pages = {539--547},
  numpages = {9},
  keywords = {Class imbalance, Classification, Precision, Recall, Data linkage, Entity resolution}
}

@article{4138bca6-f7b7-3af8-a96c-5e2544823c5c,
 ISSN = {02768739, 15206688},
 URL = {http://www.jstor.org/stable/41653827},
 abstract = {Innovation inducement prizes have been used for centuries. In the United States, a recent federal policy change—the America COMPETES Reauthorization Act of 2010—clarified and simplified a path by which all federal agencies can offer innovation inducement prizes, thus intensifying interest in how government agencies can most effectively design and apply such prizes. This paper aims to review and synthesize the academic literature on innovation inducement prizes, to clarify what has been learned that is relevant to current policy discussions, and to highlight unresolved questions that would be fruitful areas for future academic research and policy experimentation. Relative to the existing literature, this paper aims to bridge two gaps. First, I synthesize the academic literature in this area with an eye toward drawing lessons for the types of innovation inducement prizes under consideration by federal agencies under the America COMPETES Reauthorization Act. Second, I discuss the problem of how to evaluate the success or failure of innovation inducement prizes, arguing that careful empirical evaluations of innovation inducement prizes are needed in order to provide guidance to federal agencies (and others) on how to most effectively apply and design innovation inducement prizes.},
 author = {H. Williams},
 journal = {Journal of Policy Analysis and Management},
 number = {3},
 pages = {752--776},
 publisher = {Wiley},
 title = {Innovation inducement prizes: Connecting research to policy},
 urldate = {2023-08-21},
 volume = {31},
 year = {2012}
}

@inproceedings{macavaney-etal-2021-community,
  title = {Community-level Research on Suicidality Prediction in a Secure Environment: Overview of the {CLP}sych 2021 Shared Task},
  author = {MacAvaney, S. and Mittu, A. and Coppersmith, G. and Leintz, J. and Resnik, P.},
  booktitle = {Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access},
  month = jun,
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.clpsych-1.7},
  doi = {10.18653/v1/2021.clpsych-1.7},
  pages = {70--80},
}

@inproceedings{tsakalidis-etal-2022-overview,
  title = {Overview of the {CLP}sych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts},
  author = {Tsakalidis, A. and Chim, J. and Bilal, I. M. and Zirikly, A. and Atzil-Slonim, D. and Nanni, F. and Resnik, P. and Gaur, M. and Roy, K. and Inkster, B. and Leintz, J. and Liakata, M.},
  booktitle = {Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology},
  month = jul,
  year = {2022},
  address = {Seattle, USA},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2022.clpsych-1.16},
  doi = {10.18653/v1/2022.clpsych-1.16},
  pages = {184--198}
}

@article{10.1145/3433638,
  author = {Domingo-Ferrer, J. and S\'{a}nchez, D. and Blanco-Justicia, A.},
  title = {The Limits of Differential Privacy (and Its Misuse in Data Release and Machine Learning)},
  year = {2021},
  issue_date = {July 2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {64},
  number = {7},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/3433638},
  doi = {10.1145/3433638},
  abstract = {Differential privacy is not a silver bullet for all privacy problems.},
  journal = {Communications of the ACM},
  month = {jun},
  pages = {33--35},
  numpages = {3}
}

@inproceedings{van_riper,
  title = {Differential Privacy and Racial Residential Segregation},
  author = {D. {Van Riper} and T. Kugler and J. Schroeder and S. Ruggles},
  booktitle = {2020 APPAM Fall Research Conference},
  year = {2020},
}

@article{10.1257/pandp.20191107,
  Author = {Ruggles, S. and Fitch, C. and Magnuson, D. and Schroeder, J.},
  Title = {Differential Privacy and Census Data: Implications for Social and Economic Research},
  Journal = {AEA Papers and Proceedings},
  Volume = {109},
  Year = {2019},
  Month = {May},
  Pages = {403-08},
  DOI = {10.1257/pandp.20191107},
  URL = {https://www.aeaweb.org/articles?id=10.1257/pandp.20191107}
}

@misc{giles2022faking,
  title = {Faking feature importance: A cautionary tale on the use of differentially-private synthetic data}, 
  author = {O. Giles and K. Hosseini and G. Mingas and O. Strickson and L. Bowler and C. {Rangel Smith} and H. Wilde and J. {Ning Lim} and B. Mateen and K. Amarasinghe and R. Ghani and A. Heppenstall and N. Lomax and N. Malleson and M. O'Reilly and S. Vollmerteke},
  year = {2022},
  eprint = {2203.01363},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}