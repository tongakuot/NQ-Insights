---
title: "Food for Thought: The value of competitions for confidential data"
description: |
  The Food for Thought Challenge attracted new eyes from computer science and data science to think about how to address a critical real-world data linkage problem. And, in identifying different ways of addressing the same problem, it created an environment for new innovative ideas.     
categories:
  - Machine learning
  - Natural language processing
  - Public policy
  - Health and wellbeing
author: Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik 
date: 08/21/2023
toc: false
bibliography: references.bib
image: images/06-value-of-competitions.png
image-alt: Close up of shopping trolleys linked together.
---
We are witnessing a sea change in data collection practices by both governments and businesses – from purposeful collection (through surveys and censuses, for example) to opportunistic (drawing on web and social media data, and administrative datasets). This shift has made clear the importance of record linkage – a government might, for example, look to link records held by its various departments to understand how citizens make use of the gamut of public services. 

However, creating manual linkages between datasets can be prohibitively expensive, time consuming, and subject to human constraints and bias. Machine learning (ML) techniques offer the potential to combine data better, faster, and more cheaply. But, as the recently released [National AI Research Resources Task Force report](https://www.ai.gov/wp-content/uploads/2023/01/NAIRR-TF-Final-Report-2023.pdf) highlights, it is important to have an open and transparent approach to ensure that unintended biases do not occur. 

In other words, ML tools are not a substitute for thoughtful analysis. Both private and public producers of a linked dataset have to determine the level of linkage quality – such as what precision/recall tradeoff is best for the intended purpose (that is, the balance between false-positive links and failure to cover links that should be there), how much processing time and cost is acceptable, and how to address coverage issues. The challenge is made more difficult by the idiosyncrasies of heterogeneous datasets, and more difficult yet when datasets to be linked include confidential data [@10.1257/jel.20171350; @DBLP:books/sp/ChristenRS20].  

And, of course, an ML solution is never the end of the road: many data linkage scenarios are highly dynamic, involving use cases, datasets, and technical ecosystems that change and evolve over time; effective use of ML in practice necessitates an ongoing and continuous investment [@DBLP:journals/corr/abs-2112-01716]. Because techniques are constantly improving, producers need to keep abreast of new approaches. A model that is working well today may no longer work in a year because of changes in the data, or because the organizational needs have changed so that a certain type of error is no longer acceptable. As Sculley et al. point out, “it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning” [@43146].

Also important is that record linkage is not seen as a technical problem relegated to the realm of computer scientists to solve. The full engagement of domain experts in designing the optimization problem, identifying measures of success, and evaluating the quality of the results is absolutely critical, as is building an understanding of the pros and cons of different measures [@10.1371/journal.pone.0249833; @10.1007/s11222-017-9746-6]. There will need to be much learning by doing in “sandbox” environments, and back and forth communication across communities to achieve successful outcomes, as noted in the [recommendations of the Advisory Committee on Data for Evidence Building](https://www.bea.gov/system/files/2022-10/acdeb-year-2-report.pdf) (a screenshot of which is shown in Figure 1).  

[![](images/pt6-fig1.png){width="700px"}](images/pt6-fig1.png)

::: figure-caption
**Figure 1:** A recommendation for building an “innovation sandbox” as part of the creation of a new National Secure Data Service in the United States.  
:::

Despite the importance of trial and error and transparency about linkage quality, there is no handbook that guides domain experts in how to design such sandboxes. There is a very real need for agreed-upon, domain-independent guidelines, or better yet, official standards to evaluate sandboxes. Those standards would define "who" could and would conduct the evaluation, and help guarantee independence and repeatability. And while innovation challenges have been embraced by the federal government, the devil can be very much in the details [@4138bca6-f7b7-3af8-a96c-5e2544823c5c]. 

It is for this reason that the approach taken in the Food for Thought linkage competition, and described in this compendium, provides an important first step towards a well specified, replicable framework for achieving high quality outcomes. In that respect it joins other recent efforts to bring together community-level research on shared sensitive data [@macavaney-etal-2021-community; @tsakalidis-etal-2022-overview]. This competition, like those, helped bring to the foreground both the opportunities and challenges of doing research in secure sandboxes with sensitive data. Notably, these exercises highlight a kind of cultural tension between secure, managed environments, on the one hand, and unfettered machine learning research, on the other. The need for flexibility and agility in computational research bumps up against the need for advance planning and careful step-by-step processes in environments with well-defined data governance rules, and one of the key lessons learned is that the tradeoffs here need to be recognized and planned for. 

This particular competition was important for a number of other reasons. Thanks to its organization as a competition, complete with prizes and bragging rights for strongly performing teams, it attracted new eyes from computer science and data science to think about how to address a critical real-world linkage problem. It offered the potential to produce approaches that were scalable, transparent, and reproducible. The engagement of domain experts and statisticians meant that it will be possible to conduct an informed error analysis, to explicitly relate the performance metrics in the task to the problem being solved in the real world, and to bring in the expertise of survey methodologists to think about the possible adjustments. And because it identified different approaches of addressing the same problem, it created an environment for new innovative ideas.  

More generally, in addition to the excitement of the new approaches, this exercise laid bare the fragility of linkages in general and highlighted the importance of secure sandboxes for confidential data. While the promise of privacy preserving technologies is alluring as [an alternative to bringing confidential data together in one place](https://www.bea.gov/system/files/2022-10/acdeb-year-2-report.pdf), such approaches are likely too immature to deploy ad hoc until a better understanding is established of how to translate real-world problems and their associated data into well-defined tasks, how to measure quality, and particularly how to assess the impact of match quality on different subgroups [@10.1145/3433638]. The scientific profession has gone through too painful a lesson with the premature application of differential privacy techniques to ignore the lessons that can be learned from a careful and systematic analysis of different approaches [-@10.1145/3433638; @van_riper; @10.1257/pandp.20191107; @giles2022faking]. 

We hope that the articles in this collection provide not only the first steps towards a handbook of best practices, but also an inspiration to share lessons learned, so that success can be emulated, and failures understood and avoided.  

::: nav-btn-container
::: grid
::: {.g-col-12 .g-col-sm-6}
::: {.nav-btn}
[&larr; Part 5: Third place winners](/case-studies/posts/2023/08/21/05-third-place-winners.qmd)
:::
:::

::: {.g-col-12 .g-col-sm-6}
::: {.nav-btn}
[Find more case studies](/case-studies/index.qmd)
:::
:::
:::
:::

::: further-info
::: grid
::: {.g-col-12 .g-col-md-12}
About the authors
: **Steven Bedrick** is an associate professor in Oregon Health and Science University’s Department of Medical Informatics and Clinical Epidemiology.

: **Ophir Frieder** is a professor in Georgetown University's Department of Computer Science, and in the Department of Biostatistics, Bioinformatics & Biomathematics at Georgetown University Medical Center. 

: **Julia Lane** is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative.

**Philip Resnik** holds a joint appointment as professor in the University of Maryland Institute for Advanced Computer Studies and the Department of Linguistics, and an affiliate professor appointment in computer science.  
:::

::: {.g-col-12 .g-col-md-6}
Copyright and licence
: © 2023 Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik

<a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"/><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@alexandru_tugui?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Alexandru Tugui</a> on <a href="https://unsplash.com/photos/-inuQpBGbgI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>.
  

:::
::: {.g-col-12 .g-col-md-6}
How to cite
: Bedrick, Steven, Ophir Frieder, Julia Lane, and Philip Resnik. 2023. "Food for Thought: The value of competitions for confidential data." Real World Data Science, August 21, 2023. [URL](https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/06-value-of-competitions.html)
:::
:::
:::