{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In this notebook, we read in submission file and calculate success@5 and NDCG@5 score against ground truth file. The success@5 is a simple order-irrelevant metric, which is usually used when there is only one true match among all the candidates. However we do want to encourage participants take order into account, so we also involved NDCG@5 score, which is a widely used metric for rankin tasks. \n",
    "\n",
    "Notice there is a column called confidence in the sample submission. This is the probability for each prediction. Though it is not used to calculate NDCG or Success score, we'd really appreciate it if you could involve it in your submission because it will help Westat staff to better review the results.\n",
    "\n",
    "We currently plan on using these measures, suitable for ranking problems, for model evaluation. Participating teams will be notified of any changes in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pt = 'sample_submission.csv'\n",
    "ref_pt = 'fake_ground_truth.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(gen_pt, dtype=str)\n",
    "ground_truth = pd.read_csv(ref_pt, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submission = submission.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.merge(submission, ground_truth, how='outer', on='upc')\n",
    "df = df[df['ec_y'].notna()]\n",
    "df['correct'] = df['ec_x'] == df['ec_y']\n",
    "df['correct'] = df['correct'].astype(int)\n",
    "df = df.groupby('upc').agg(lambda x: x.tolist()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k=5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r: Relevance scores list (binary value) in rank order\n",
    "        k: Number of results to consider\n",
    "\n",
    "    Returns:\n",
    "        Discounted Cumulative Gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2))) \n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k=5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r: Relevance scores list (binary value) in rank order\n",
    "        k: Number of results to consider\n",
    "    Returns:\n",
    "        Normalized Discounted Cumulative Gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "\n",
    "def success_at_k(r, k=5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r: Correct match list (binary value) in rank order\n",
    "        k: Number of results to consider\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    return np.sum(r[:k])\n",
    "\n",
    "def padding(r, k=5):\n",
    "    while len(r) < k:\n",
    "        r.append(0)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We used padding here to make sure every UPC has at least 5 predictions. \n",
    "# If there are less than 5 predictions, it will be filled with 0 to make the score consistent.\n",
    "df['correct'] = df['correct'].apply(padding)\n",
    "df['ndcg@5'] = df['correct'].apply(ndcg_at_k)\n",
    "df['s@5'] = df['correct'].apply(success_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of the scoring details\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ndcg_score = df['ndcg@5'].mean()\n",
    "success_score = df['s@5'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The NDCG@5 score is: {}\".format(round(ndcg_score, 3)))\n",
    "print(\"The Success@5 score is: {}\".format(round(success_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Help Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with the process of evaluation, we attached another python file in this folder. To use it, simply run in the command line: \n",
    " - `python evaluation_script.py <submission_csv_file_path> <ground_truth_csv_file_path>`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to output the files everytime and want to see the scores inside your notebook. You could import `evaluate` function to you notebook from the script and use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_script import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(submission, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please let us know if you find any issues with the evaluation script. And we will keep you posted if there is any update in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
